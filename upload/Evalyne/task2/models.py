# -*- coding: utf-8 -*-
"""Untitled6.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/18fJpLopnVuX7pmn9GWD2Mtr0OmIONgy9
"""

from google.colab import drive
drive.mount('/content/drive')

import numpy
import json
import pandas as pd
from keras.models import Sequential
from keras.layers import Dense, Input, LSTM, Embedding, Dropout, GRU
from keras.layers import Bidirectional
import tensorflow as tf
!pip install transformers
import torch
import numpy

# setup data
!wget -q https://raw.githubusercontent.com/Alegzandra/RED-Romanian-Emotions-Dataset/main/REDv2/data/train.json
!wget -q https://raw.githubusercontent.com/Alegzandra/RED-Romanian-Emotions-Dataset/main/REDv2/data/valid.json
!wget -q https://raw.githubusercontent.com/Alegzandra/RED-Romanian-Emotions-Dataset/main/REDv2/data/test.json

with open("train.json", "r", encoding="utf8") as f:
  train_dataset = json.load(f)
with open("valid.json", "r", encoding="utf8") as f:
  valid_dataset = json.load(f)
with open("test.json", "r", encoding="utf8") as f:
  test_dataset = json.load(f)

class MyModel():
  def __init__(self):
    # do here any initializations you require
    self.df_train_dataset = self.create_df(train_dataset)
    self.df_test_dataset = self.create_df(test_dataset)
    self.df_valid_dataset = self.create_df(valid_dataset)
    self.embedding_matrix = None
    self.train_data = None
    self.vocab_size = 4088
    self.max_length = 105
    self.test_data = self.generate(self.df_test_dataset)
    self.validate_data =self.generate(self.df_valid_dataset)
    self.train_labels = numpy.array(self.df_train_dataset['agreed_lables'].tolist())
    self.validate_labels = numpy.array(self.df_val_dataset['agreed_lables'].tolist())
    


  

  def create_df(self, dataset):
    text = []
    labels = []
    for item in dataset:
      text.append(item['text'])
      labels.append(item['agreed_labels'])
      # convert the lists into a dataframe
      return pd.DataFrame(list(zip(text, labels)), columns=['text','agreed_lables'])


  def generate(self, dataset):
    # pytorch
    from transformers import AutoModel, AutoTokenizer, AutoModel
    tokenizer = AutoTokenizer.from_pretrained("readerbench/RoBERT-small")
    model = AutoModel.from_pretrained("readerbench/RoBERT-small")

    data = []
    sentences = dataset['text'].tolist()
    for sentence in sentences:
      inputs = tokenizer(sentence, return_tensors="pt")
      data.append(inputs['input_ids'].detach().numpy())

    new_list =  []
    for arr in data:
      lst = arr.tolist()
      new_list.append(lst[0])

    

    import tensorflow as tf
    padding= tf.keras.preprocessing.sequence.pad_sequences(new_list, padding= 'post', maxlen= 105)
  

    return padding

  def load(self, model_resource_folder):
    # we'll call this code before prediction
    # use this function to load any pretrained model and any other resource, from the given folder path
    #load embeddings matrix
    self.embedding_matrix = numpy.load('/content/data.npy')
    #load input matrix
    self.train_data = numpy.load('/content/data_inputs.npy')
    print(self.embedding_matrix.shape)
    print(self.train_data.shape)

  def train(self, train_json_file, validation_json_file, model_resource_folder):
    # we'll call this function right after init
    # place here all your training code
    # at the end of training, place all required resources, trained model, etc in the given model_resource_folder
    #Defining MyModel
    model_BIGRU = Sequential()
    e = Embedding(self.vocab_size, 256, weights=[self.embedding_matrix], input_length=self.max_length, trainable=False) #false means I have my trained weights already
    model_BIGRU.add(e)
    model_BIGRU.add(Bidirectional(GRU(95)))
    model_BIGRU.add(Dense(22))
    model_BIGRU.add(Dense(7, activation='softmax'))

    model_BIGRU.compile(loss='binary_crossentropy', optimizer='Adam',metrics=['accuracy'])

    model_BIGRU.summary()

    

    # fit the BIGRU model
    GRU_model = model_BIGRU.fit(self.train_data, self.train_labels, epochs=40, validation_data=(self.validate_data, self.validate_labels), batch_size=150, shuffle=True)

  #def predict(self, test_json_file):
    # we'll call this function after the load()
    # use this place to run the prediction
    # the output of this function is a single value, the Hamming loss on the given json file

model = MyModel()
model.load('/content')

# TRAINING
model = MyModel()
model.train(train_dataset, valid_dataset, "output")

# INFERENCE
from time import perf_counter 

# load model  
model = MyModel()
model.load("output")

# inference
start_time = perf_counter()
# right after we'll reload your model from the output folder
hamming_loss = model.predict(test_dataset)
stop_time = perf_counter()

print(f"Predicted in {stop_time-start_time:.2s}.")
print(f"Hamming loss = {hamming_loss}")  # this is the score we want :)